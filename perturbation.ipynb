{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605f8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both benign documents \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b6aff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/anp407/.cache/conda_envs/unknownattack/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load embedding model library\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "# load qwen 0.6b embedding\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM \n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded models, qwen embedding and gemma 3 4b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load qwen 0.6B embedding \n",
    "e_tokenizer = AutoTokenizer.from_pretrained(\"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-Embedding-0.6B\", trust_remote_code=True)\n",
    "e_model = AutoModel.from_pretrained(\"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-Embedding-0.6B\", trust_remote_code=True, device_map=\"auto\", dtype=torch.float16)\n",
    "e_model.train() # we need the gradients\n",
    "# use gemma 3 4b it to generate the answer\n",
    "g_tokenizer = AutoTokenizer.from_pretrained(\"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-4B-Instruct-2507\", trust_remote_code=True)\n",
    "g_model = AutoModelForCausalLM.from_pretrained(\"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-4B-Instruct-2507\", trust_remote_code=True, device_map=\"auto\", dtype=torch.float16)\n",
    "g_model.eval() # inference only\n",
    "print(\"loaded models, qwen embedding and Qwen3 4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6ab7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_documents = json.load(open(\"nq_gemini_enriched.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601b5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random a golden trigger\n",
    "import random\n",
    "# position is begin, mid and end\n",
    "positions = [\"begin\", \"mid\", \"end\"]\n",
    "num_tokens = [5, 10, 15]\n",
    "target_gradient_guidance = True\n",
    "device = 'cuda:0'\n",
    "p_steps = 10\n",
    "# p_target = ['query', 'document']\n",
    "p_target = ['query']\n",
    "steer = '-1'\n",
    "\n",
    "queries = []\n",
    "for key in adv_documents:\n",
    "    queries.append(adv_documents[key]['question'])\n",
    "benign_docs = []\n",
    "for key in adv_documents:\n",
    "    benign_docs.append(adv_documents[key]['benign_responses'])\n",
    "adv_docs = []\n",
    "for key in adv_documents:\n",
    "    adv_docs.append(adv_documents[key]['adv_texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79707fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_2715574/1722610131.py:55: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  trigger_embeds -= trigger_embeds.grad * 0.1\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m steer == \u001b[33m'\u001b[39m\u001b[33m-1\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     54\u001b[39m     trigger_embeds = e_model.get_input_embeddings()(trigger_ids)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     trigger_embeds -= \u001b[43mtrigger_embeds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\n\u001b[32m     56\u001b[39m     e_model.get_input_embeddings().weight[trigger_ids.squeeze()] = trigger_embeds.squeeze()\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m steer == \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "for pos in positions:\n",
    "    for num_token in num_tokens:\n",
    "        # we tried different positions and num_tokens, here we just use one example\n",
    "        # we have only one adv passage, but 10 benign passages, we want to add a trigger both to the adv passages and query to increase its similarity\n",
    "        \n",
    "        # if the p_target is query, we only add trigger to the query\n",
    "        if 'query' in p_target:\n",
    "            for i, query in tqdm(enumerate(queries)):\n",
    "                trigger_ids = [e_tokenizer.pad_token_id] * num_token\n",
    "                trigger_token_list = e_tokenizer.convert_ids_to_tokens(trigger_ids)\n",
    "                trigger_ids = torch.tensor(trigger_ids, device='cuda:0').unsqueeze(0)\n",
    "                trigger_attention_mask = torch.ones_like(trigger_ids, device='cuda:0')\n",
    "                \n",
    "                query_ids = e_tokenizer(query, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "                if pos == \"begin\":\n",
    "                    p_query_ids = torch.cat([trigger_ids, query_ids['input_ids']], dim=1)\n",
    "                    p_query_attention_mask = torch.cat([trigger_attention_mask, query_ids['attention_mask']], dim=1)\n",
    "                    p_query_ids_fixed = \n",
    "                elif pos == \"mid\":\n",
    "                    mid_index = query_ids['input_ids'].size(1) // 2\n",
    "                    p_query_ids = torch.cat([query_ids['input_ids'][:, :mid_index], trigger_ids, query_ids['input_ids'][:, mid_index:]], dim=1)\n",
    "                    p_query_attention_mask = torch.cat([query_ids['attention_mask'][:, :mid_index], trigger_attention_mask, query_ids['attention_mask'][:, mid_index:]], dim=1)\n",
    "                elif pos == \"end\":\n",
    "                    p_query_ids = torch.cat([query_ids['input_ids'], trigger_ids], dim=1)\n",
    "                    p_query_attention_mask = torch.cat([query_ids['attention_mask'], trigger_attention_mask], dim=1)\n",
    "                    \n",
    "                # get the query_embeddings\n",
    "                p_query_embeddings = e_model(input_ids=p_query_ids, attention_mask=p_query_attention_mask)\n",
    "                \n",
    "                # get the document embeddings\n",
    "                b_embeddings = []\n",
    "                for doc in benign_docs[i]:\n",
    "                    doc_ids = e_tokenizer(doc, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "                    doc_embeddings = e_model(input_ids=doc_ids['input_ids'], attention_mask=doc_ids['attention_mask'])\n",
    "                    b_embeddings.append(doc_embeddings)\n",
    "                a_doc_ids = e_tokenizer(adv_docs[i][0], add_special_tokens=False, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "                a_doc_embeddings = e_model(input_ids=a_doc_ids['input_ids'], attention_mask=a_doc_ids['attention_mask'])\n",
    "            \n",
    "                # iterate p_steps to optimize the trigger\n",
    "                q_embedding_before = e_model.get_input_embeddings()(p_query_ids)\n",
    "                q_embedding_after = e\n",
    "                 \n",
    "                for step in range(p_steps):\n",
    "                    # compute similarity loss\n",
    "                    b_min_similarities = []\n",
    "                    for b_emb in b_embeddings:\n",
    "                        similarity = torch.cosine_similarity(p_query_embeddings.last_hidden_state[:,-1,:], b_emb.last_hidden_state[:,-1,:])\n",
    "                        b_min_similarities.append(similarity)\n",
    "                    b_min_sim = torch.min(torch.stack(b_min_similarities))\n",
    "                    \n",
    "                    a_similarity = torch.cosine_similarity(p_query_embeddings.last_hidden_state[:,-1,:], a_doc_embeddings.last_hidden_state[:,-1,:])\n",
    "                    # compute loss\n",
    "                    loss = - a_similarity + b_min_sim\n",
    "                    # backpropagate\n",
    "                    loss.backward()\n",
    "                    # get the gradient\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running optimization with GCG\n",
    "ids_suffix_best = ids_suffix.clone()\n",
    "best_loss = float(\"inf\")\n",
    "all_losses = []\n",
    "for step in tqdm(range(steps), desc=\"Optimization steps\", unit=\"step\"):\n",
    "    # Getting input embeds of current suffix\n",
    "    one_hot = torch.nn.functional.one_hot(ids_suffix, num_classes=model.config.vocab_size).to(model.device, model.dtype)\n",
    "    one_hot.requires_grad = True\n",
    "    embeds_suffix = one_hot @ model.get_input_embeddings().weight\n",
    "\n",
    "    # Getting gradients w.r.t one-hot encodings\n",
    "    cache_copy = deepcopy(kv_cache) # In recent versions of huggingface's transformers, we need a copy of the cache to avoid getting gradients multiple times w.r.t the same tensors\n",
    "    loss = model(\n",
    "        inputs_embeds=torch.cat([embeds_suffix, embeds_after], dim=1),\n",
    "        labels=labels,\n",
    "        past_key_values=cache_copy,\n",
    "        use_cache=True\n",
    "    ).loss\n",
    "    loss.backward()\n",
    "    gradients = -one_hot.grad\n",
    "    \n",
    "    # Updating best suffix ever\n",
    "    all_losses.append(loss.item())\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        ids_suffix_best = ids_suffix.clone()\n",
    "\n",
    "    # Getting top-k tokens for all positions (candidate substitutions)\n",
    "    top_k_tokens = torch.topk(gradients, top_k, dim=-1).indices\n",
    "\n",
    "    # Creating a batch with substitutions and storing losses\n",
    "    sub_positions = torch.randint(0, suffix_length, (batch_size,))\n",
    "    sub_tokens = torch.randint(0, top_k, (batch_size,))\n",
    "    batch = ids_suffix.clone().repeat(batch_size, 1)\n",
    "    for idx, (position, token) in enumerate(zip(sub_positions, sub_tokens)):\n",
    "        batch[idx, position] = top_k_tokens[0, position, token]\n",
    "\n",
    "    # Computing losses for the batch (in sub mini-batches)\n",
    "    losses = []\n",
    "    for slice_start in range(0, batch_size, search_batch_size):\n",
    "        slice_end = min(slice_start + search_batch_size, batch_size)\n",
    "        ids_slice = batch[slice_start: slice_end]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Getting loss for the batch\n",
    "            try:\n",
    "                batch_kv_cache_copy = deepcopy(batch_kv_cache)\n",
    "                logits = model(\n",
    "                    input_ids=torch.cat([ids_slice, ids_after.repeat(ids_slice.shape[0], 1)], dim=1),\n",
    "                    past_key_values=batch_kv_cache_copy,\n",
    "                    use_cache=True\n",
    "                ).logits[:, -ids_after.shape[1]: -1]\n",
    "\n",
    "                # Getting losses\n",
    "                losses.extend([\n",
    "                    torch.nn.functional.cross_entropy(logits[i], ids_after[0, 1:]).item()\n",
    "                    for i in range(search_batch_size)\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e}\")\n",
    "                print(\"Exception during forward pass. If OOM, try reducing the search batch size.\")\n",
    "                break\n",
    "\n",
    "    # Updating the suffix\n",
    "    best_idx = np.argmin(losses)\n",
    "    best_position, best_token = sub_positions[best_idx].item(), sub_tokens[best_idx].item()\n",
    "    ids_suffix[0, best_position] = top_k_tokens[0, best_position, best_token]\n",
    "\n",
    "    # Logging infos\n",
    "    mean_loss = np.mean(losses)\n",
    "    print(f\"Step {step + 1}/{steps} | Best loss: {best_loss:.3f} | Current loss: {loss.item():.3f} | Mean loss: {mean_loss}\")\n",
    "    visualize(ids_before, ids_suffix, ids_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f3de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_placeholder = '|SUFFIX_PLACEHOLDER|'\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"system_prompt\"},\n",
    "    {\"role\": \"user\", \"content\": \"request\" + '|SUFFIX_PLACEHOLDER|'},\n",
    "    {\"role\": \"assistant\", \"content\": \"target\"},\n",
    "]\n",
    "\n",
    "# Getting text before and after the suffix\n",
    "text = e_tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "text_before, text_after = text.split(suffix_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c29b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "system_prompt<|im_end|>\n",
      "<|im_start|>user\n",
      "request\n",
      "------\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "target<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_before )\n",
    "print('------')\n",
    "print(text_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f1a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0043,  0.0435, -0.0334,  ..., -0.0039,  0.0449,  0.0320],\n",
       "         [-0.0043,  0.0435, -0.0334,  ..., -0.0039,  0.0449,  0.0320],\n",
       "         [-0.0043,  0.0435, -0.0334,  ..., -0.0039,  0.0449,  0.0320],\n",
       "         [-0.0043,  0.0435, -0.0334,  ..., -0.0039,  0.0449,  0.0320],\n",
       "         [-0.0043,  0.0435, -0.0334,  ..., -0.0039,  0.0449,  0.0320]]],\n",
       "       device='cuda:1', dtype=torch.float16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigger_embeds.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4337722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPast(last_hidden_state=tensor([[[  3.3613,  -7.2148,  -0.1650,  ...,  -8.4219, -12.9375,   0.3081],\n",
       "         [  3.3613,  -7.2148,  -0.1650,  ...,  -8.4219, -12.9375,   0.3081],\n",
       "         [  3.3613,  -7.2148,  -0.1650,  ...,  -8.4219, -12.9375,   0.3081],\n",
       "         ...,\n",
       "         [ -1.7715,  -1.2158,  -0.2764,  ...,   2.0938,  -3.4062,  -0.3687],\n",
       "         [ -1.4912,  -1.7412,  -0.6958,  ...,   1.3066,  -2.0957,  -1.4893],\n",
       "         [ -3.2285,  -5.7148,  -0.4097,  ...,  -1.7334,  -7.3594,  -1.2129]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_query_embeddings.hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unknownattack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
