{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa829c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/anp407/.cache/conda_envs/unknownattack/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import colorama\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed, AutoModel\n",
    "from transformers.cache_utils import DynamicCache\n",
    "from datasets import load_dataset\n",
    "import huggingface_hub\n",
    "\n",
    "# Setting reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\" # Tough cookie! Also, requires permissions through HF authentication\n",
    "model_name = \"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-Embedding-0.6B\"\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Attack parameters\n",
    "batch_size = 512 # Number of samples to optimize over (512 in GCG paper)\n",
    "search_batch_size = 256 # Number of samples that actually run forward together\n",
    "top_k = 256 # Number of top tokens to sample from (256 in GCG paper)\n",
    "steps = 100 # Total number of optimization steps (500 in GCG paper)\n",
    "suffix_initial_token = \" !\" # Initial token repeated for the length of the suffix\n",
    "device_name = \"cuda:0\"\n",
    "\n",
    "# Assertions\n",
    "assert batch_size % search_batch_size == 0, \"Batch size must be divisible by search batch size (convenience)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19ad909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model and tokenizer\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_name,\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf1dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "# dataset = load_dataset(\"walledai/AdvBench\", split='train')\n",
    "import json\n",
    "dataset = json.load(open(\"nq_gemini_enrich_10_gemma_verified_final.json\"))\n",
    "\n",
    "# Getting request and target\n",
    "queries = [dataset[x]['question'] for x in dataset]\n",
    "\n",
    "correct_answers = [dataset[x]['correct answer'] for x in dataset]\n",
    "incorrect_answers = [dataset[x]['incorrect answer'] for x in dataset]\n",
    "adv_docs = [dataset[x]['filtered_adv_responses'] + dataset[x]['extra_filtered_adv_responses'] for x in dataset]\n",
    "benign_docs = [dataset[x]['filtered_benign_responses'] + dataset[x]['extra_filtered_benign_responses'] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1704349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [\"end\"]\n",
    "lengths = [1,2,4,8,16]\n",
    "loss_type = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read existing results if any\n",
    "import os\n",
    "if os.path.exists('gcg_all_result.jsonl'):\n",
    "    all_result = {}\n",
    "    with open('gcg_all_result.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            for k in entry:\n",
    "                all_result[eval(k)] = entry[k]\n",
    "else:\n",
    "    all_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd010b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization steps:  14%|█▍        | 14/100 [00:02<00:16,  5.30step/s]"
     ]
    }
   ],
   "source": [
    "# Running optimization with GCG\n",
    "# for 100 samples\n",
    "# record the best _result\n",
    "# early_stop_thred = 0.1\n",
    "# tolerance_step = 30\n",
    "succeed_step = -1\n",
    "from torch import Tensor \n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    # left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    # if left_padding:\n",
    "    #     return last_hidden_states[:, -1]\n",
    "    # else:\n",
    "    sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "    batch_size = last_hidden_states.shape[0]\n",
    "    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "for position in positions:\n",
    "    for length in lengths:\n",
    "        for i, query in enumerate(queries):\n",
    "            succeed_step = -1\n",
    "            if (i, position, length, loss_type) in all_result:\n",
    "                print(f\"Skipping already processed (i={i}, position={position}, length={length}, loss_type={loss_type})\")\n",
    "                continue\n",
    "            # check the rerun label\n",
    "\n",
    "            all_result[(i, position, length, loss_type)] = {}\n",
    "            all_result[(i, position, length, loss_type)]['loss_history'] = []\n",
    "            all_result[(i, position, length, loss_type)]['ids_suffix_history'] = []\n",
    "            \n",
    "            # Fix variable shadowing and indexing\n",
    "            current_adv_docs = adv_docs[i]\n",
    "            current_benign_docs = benign_docs[i]\n",
    "            \n",
    "            # The user code had: adv_docs = adv_docs[i]  # only use the first adv doc for now\n",
    "            # Assuming they want the first document from the list of documents for this query\n",
    "            if isinstance(current_adv_docs, list) and len(current_adv_docs) > 0:\n",
    "                target_adv_docs = [current_adv_docs[0]]\n",
    "            else:\n",
    "                target_adv_docs = [current_adv_docs] if isinstance(current_adv_docs, str) else current_adv_docs\n",
    "\n",
    "            if isinstance(current_benign_docs, list) and len(current_benign_docs) > 0:\n",
    "                target_benign_docs = current_benign_docs # Use all?\n",
    "            else:\n",
    "                target_benign_docs = [current_benign_docs] if isinstance(current_benign_docs, str) else current_benign_docs\n",
    "            \n",
    "            if position == \"start\":\n",
    "                text_before = \"\"\n",
    "                text_mid = \" !\" * length\n",
    "                text_after = query\n",
    "            elif position == \"mid\":\n",
    "                text_before = query[:len(query)//2]\n",
    "                text_mid = \" !\" * length\n",
    "                text_after = query[len(query)//2:]\n",
    "            elif position == \"end\":\n",
    "                text_before = query\n",
    "                text_mid = \" !\" * length\n",
    "                text_after = \"\"\n",
    "\n",
    "            ids_before = tokenizer(text_before, return_tensors=\"pt\", add_special_tokens=False).to(model.device)['input_ids']\n",
    "            ids_mid = tokenizer(text_mid, return_tensors=\"pt\", add_special_tokens=False).to(model.device)['input_ids']\n",
    "            ids_after = tokenizer(text_after, return_tensors=\"pt\", add_special_tokens=False).to(model.device)['input_ids']\n",
    "            \n",
    "            # Initialize suffix ids from ids_mid\n",
    "            ids_suffix = ids_mid.clone()\n",
    "\n",
    "            # Pre-compute doc embeddings\n",
    "            adv_doc_ids = tokenizer(target_adv_docs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)['input_ids']\n",
    "            with torch.no_grad():\n",
    "                adv_doc_embeds = last_token_pool(model(adv_doc_ids, output_hidden_states=True).last_hidden_state, adv_doc_ids != tokenizer.pad_token_id)\n",
    "            \n",
    "            benign_doc_ids = tokenizer(target_benign_docs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)['input_ids']\n",
    "            with torch.no_grad():\n",
    "                benign_doc_embeds = last_token_pool(model(benign_doc_ids, output_hidden_states=True).last_hidden_state, benign_doc_ids != tokenizer.pad_token_id)\n",
    "            ids_suffix_best = ids_suffix.clone()\n",
    "            best_loss = float(\"inf\")\n",
    "            all_losses = []\n",
    "            \n",
    "            for step in tqdm(range(steps), desc=\"Optimization steps\", unit=\"step\"):\n",
    "                # 1. Gradient Step\n",
    "                embeds_before = model.get_input_embeddings()(ids_before)\n",
    "                embeds_after = model.get_input_embeddings()(ids_after)\n",
    "                \n",
    "                one_hot = torch.nn.functional.one_hot(ids_suffix, num_classes=model.config.vocab_size).to(model.device, model.dtype)\n",
    "                one_hot.requires_grad = True\n",
    "                embeds_suffix = one_hot @ model.get_input_embeddings().weight\n",
    "                \n",
    "                full_embeds = torch.cat([embeds_before, embeds_suffix, embeds_after], dim=1)\n",
    "                \n",
    "                outputs = model(inputs_embeds=full_embeds, output_hidden_states=True)\n",
    "                q_embeds = last_token_pool(outputs.last_hidden_state, ids_before != tokenizer.pad_token_id)\n",
    "                \n",
    "                # plan 0s: simple simliarity difference\n",
    "                if loss_type == 0:\n",
    "                    sim_adv = torch.cosine_similarity(q_embeds, adv_doc_embeds, dim=-1).min() # more the better\n",
    "                    sim_benign = torch.cosine_similarity(q_embeds, benign_doc_embeds, dim=-1).max() # less the better\n",
    "                    loss = - sim_adv + sim_benign\n",
    "                # print(f\"Step {step}: Loss = {loss.item()}, Sim_adv = {sim_adv.item()}, Sim_benign = {sim_benign.item()}\")\n",
    "                # plan1: contrastive loss contrastive learning loss https://lilianweng.github.io/posts/2021-05-31-contrastive/ \n",
    "                elif loss_type == 1:\n",
    "                    temp = 0.07 # use common temperature https://openreview.net/forum?id=ejHUr4nfHhD\n",
    "                    sim_adv = torch.cosine_similarity(q_embeds, adv_doc_embeds, dim=-1)\n",
    "                    sim_benign = torch.cosine_similarity(q_embeds, benign_doc_embeds, dim=-1)\n",
    "                    logits = torch.cat([sim_adv, sim_benign], dim=0) / temp # sim_adv more the better, sim_benign less the better\n",
    "                    labels = torch.zeros(logits.size(0), dtype=torch.float).to(model.device)\n",
    "                    labels[0] += 1  # First half are adv, second half benign\n",
    "                    loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "                # plan2: qwen training loss https://arxiv.org/pdf/2506.05176.pdf\n",
    "                elif loss_type == 2:\n",
    "                    temp = 0.07\n",
    "                    sim_adv = torch.cosine_similarity(q_embeds, adv_doc_embeds, dim=-1)/ temp\n",
    "                    sim_benign = torch.cosine_similarity(q_embeds, benign_doc_embeds, dim=-1)/ temp\n",
    "                    sim_adv_benign = torch.cosine_similarity(adv_doc_embeds, benign_doc_embeds, dim=-1)/ temp # fixed\n",
    "                    sim_benign_exp_mean = torch.exp(sim_benign).mean(dim=0) # less the better\n",
    "                    sim_adv_benign_exp_mean = torch.exp(sim_adv_benign).mean(dim=0) # fixed\n",
    "                    sim_adv_exp_mean = torch.exp(sim_adv).mean(dim=0) # more the better\n",
    "                    Z = sim_adv_exp_mean + sim_benign_exp_mean + sim_adv_benign_exp_mean\n",
    "                    loss = - torch.log(sim_adv_exp_mean / Z) \n",
    "                # plan 3: triplet loss\n",
    "                # margin = 0.1\n",
    "                # loss = torch.clamp(margin - sim_adv + sim_benign, min=0.0) # want sim_benign - sim_adv = 0.1\n",
    "                # plan 4: nce loss\n",
    "                # temp = 0.07\n",
    "                # logits = torch.mm(q_embeds, torch.cat([adv_doc_embeds, benign_doc_embeds], dim=0).t()) / temp\n",
    "                # labels = torch.zeros(q_embeds.size(0), dtype=torch.long).to(model.device)\n",
    "                # loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "                # plan 5: margin ranking loss\n",
    "                # margin = 0.1\n",
    "                # target = torch.ones_like(sim_adv)\n",
    "                # loss = torch.nn.MarginRankingLoss(margin=margin)(sim_adv.unsqueeze(1), sim_benign.unsqueeze(1), target)\n",
    "\n",
    "                loss.backward()\n",
    "                gradients = -one_hot.grad\n",
    "                \n",
    "                all_losses.append(loss.item())\n",
    "                if loss.item() < best_loss:\n",
    "                    best_loss = loss.item()\n",
    "                    ids_suffix_best = ids_suffix.clone()\n",
    "\n",
    "                # 2. Candidate Selection\n",
    "                top_k_tokens = torch.topk(gradients, top_k, dim=-1).indices\n",
    "                \n",
    "                sub_positions = torch.randint(0, length, (batch_size,))\n",
    "                sub_tokens = torch.randint(0, top_k, (batch_size,))\n",
    "                \n",
    "                candidate_ids = ids_suffix.clone().repeat(batch_size, 1)\n",
    "                for idx, (pos, tok_idx) in enumerate(zip(sub_positions, sub_tokens)):\n",
    "                    candidate_ids[idx, pos] = top_k_tokens[0, pos, tok_idx]\n",
    "                \n",
    "                candidate_losses = []\n",
    "                for slice_start in range(0, batch_size, search_batch_size):\n",
    "                    slice_end = min(slice_start + search_batch_size, batch_size)\n",
    "                    ids_slice = candidate_ids[slice_start: slice_end]\n",
    "                    \n",
    "                    curr_batch_size = ids_slice.shape[0]\n",
    "                    batch_ids_before = ids_before.repeat(curr_batch_size, 1)\n",
    "                    batch_ids_after = ids_after.repeat(curr_batch_size, 1)\n",
    "                    batch_input_ids = torch.cat([batch_ids_before, ids_slice, batch_ids_after], dim=1)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        batch_outputs = model(input_ids=batch_input_ids, output_hidden_states=True)\n",
    "                        batch_q_embeds = last_token_pool(batch_outputs.last_hidden_state, batch_input_ids != tokenizer.pad_token_id)\n",
    "                        \n",
    "                        batch_q_norm = torch.nn.functional.normalize(batch_q_embeds, p=2, dim=1)\n",
    "                        adv_norm = torch.nn.functional.normalize(adv_doc_embeds, p=2, dim=1)\n",
    "                        benign_norm = torch.nn.functional.normalize(benign_doc_embeds, p=2, dim=1)\n",
    "                        \n",
    "                        batch_sim_adv = torch.mm(batch_q_norm, adv_norm.t())\n",
    "                        batch_sim_benign = torch.mm(batch_q_norm, benign_norm.t())\n",
    "                        \n",
    "                        batch_loss = - batch_sim_adv.min(dim=1).values + batch_sim_benign.max(dim=1).values\n",
    "                        candidate_losses.extend(batch_loss.tolist())\n",
    "\n",
    "                best_idx = np.argmin(candidate_losses)\n",
    "                # print(f\"Step {step}: Best Loss = {candidate_losses[best_idx]}\")\n",
    "                ids_suffix = candidate_ids[best_idx].unsqueeze(0)\n",
    "                \n",
    "                min_loss = np.min(candidate_losses)\n",
    "                # if the candidate_loss is less than 0, then success and break.\n",
    "                if min_loss < 0:\n",
    "                    succeed_step = step\n",
    "                \n",
    "                all_result[(i, position, length, loss_type)]['loss_history'].append(min_loss.item())\n",
    "                all_result[(i, position, length, loss_type)]['ids_suffix_history'].append(ids_suffix.cpu().numpy())\n",
    "                all_result[(i, position, length, loss_type)]['succeed_step'] = succeed_step\n",
    "                \n",
    "                # if len(all_result[(i, position, length)]['loss_history']) > tolerance_step:\n",
    "                #     recent_losses = all_result[(i, position, length)]['loss_history'][-tolerance_step:]\n",
    "                #     if (max(recent_losses) - min(recent_losses))/ max(recent_losses) < early_stop_thred:\n",
    "                #         print(f\"Early stopping at step {step} due to minimal loss improvement.\")\n",
    "                #         break\n",
    "            \n",
    "                del embeds_before, embeds_suffix, embeds_after, full_embeds, outputs, one_hot, gradients, candidate_ids, candidate_losses\n",
    "                torch.cuda.empty_cache()\n",
    "                    \n",
    "            all_result[(i, position, length, loss_type)]['best_loss'] = best_loss\n",
    "            all_result[(i, position, length, loss_type)]['ids_suffix_best'] = ids_suffix_best.cpu().numpy()\n",
    "            # save the result\n",
    "            with open(f'gcg_all_result.jsonl', 'a') as f:\n",
    "                for sub_k in all_result[(i, position, length, loss_type)]:\n",
    "                    if torch.is_tensor(all_result[(i, position, length, loss_type)][sub_k]):\n",
    "                        all_result[(i, position, length, loss_type)][sub_k] = all_result[(i, position, length, loss_type)][sub_k].cpu().tolist()\n",
    "                    if isinstance(all_result[(i, position, length, loss_type)][sub_k], np.ndarray):\n",
    "                        all_result[(i, position, length, loss_type)][sub_k] = all_result[(i, position, length, loss_type)][sub_k].tolist()\n",
    "                    # if list\n",
    "                    if isinstance(all_result[(i, position, length, loss_type)][sub_k], list):\n",
    "                        for idx, item in enumerate(all_result[(i, position, length, loss_type)][sub_k]):\n",
    "                            if torch.is_tensor(item):\n",
    "                                all_result[(i, position, length, loss_type)][sub_k][idx] = item.cpu().tolist()\n",
    "                            if isinstance(item, np.ndarray):\n",
    "                                all_result[(i, position, length, loss_type)][sub_k][idx] = item.tolist()\n",
    "                f.write(json.dumps({str((i, position, length, loss_type)): all_result[(i, position, length, loss_type)]}) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unknownattack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
