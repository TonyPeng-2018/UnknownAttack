{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cf4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this document reads all generated documents, and check if they can provide enough misinformation to flip the model's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1a4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both benign documents \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73aed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add benign texts to adv_documents\n",
    "adv_documents = json.load(open(\"nq_gemini_enrich_10_verify.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65eb4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/anp407/.cache/conda_envs/unknownattack/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load embedding model library\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "# load qwen 0.6b embedding\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM \n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891ec9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded models, qwen embedding and gemma 3 4b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load qwen 0.6B embedding \n",
    "e_tokenizer = AutoTokenizer.from_pretrained(\"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-Embedding-0.6B\", trust_remote_code=True)\n",
    "e_model = AutoModel.from_pretrained(\"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-Embedding-0.6B\", trust_remote_code=True, device_map=\"auto\", dtype=torch.float16)\n",
    "e_model.train() # we need the gradients\n",
    "# use gemma 3 4b it to generate the answer\n",
    "g_tokenizer = AutoTokenizer.from_pretrained(\"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-4B-Instruct-2507\", trust_remote_code=True)\n",
    "g_model = AutoModelForCausalLM.from_pretrained(\"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-4B-Instruct-2507\", trust_remote_code=True, device_map=\"auto\", dtype=torch.float16)\n",
    "g_model.eval() # inference only\n",
    "print(\"loaded models, qwen embedding and gemma 3 4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ace8cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adv_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43madv_documents\u001b[49m[\u001b[38;5;28mlist\u001b[39m(adv_documents.keys())[\u001b[32m0\u001b[39m]]\n",
      "\u001b[31mNameError\u001b[39m: name 'adv_documents' is not defined"
     ]
    }
   ],
   "source": [
    "adv_documents[list(adv_documents.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8328acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [40:54<00:00, 24.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# a simple generation prompt\n",
    "simple_prompt = \"Answer the question based on the provided context.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "# a simple judge prompt\n",
    "judge_prompt = 'Given the generated answer: \"{generated_answer}\" and the original answer: \"{original_answer}\", is the generated answer the same as the original answer? Answer with a simple \"Yes\" or \"No\".\\n\\nAnswer:'\n",
    "\n",
    "# iterate all questions, check if the generated answer is different from the original answer\"\n",
    "for i, key in tqdm(enumerate(adv_documents), total=len(adv_documents)):\n",
    "    question = adv_documents[key]['question']\n",
    "    correct_answer = adv_documents[key]['correct answer']\n",
    "    incorrect_answer = adv_documents[key]['incorrect answer']\n",
    "    # check all benign contexts\n",
    "    adv_documents[key]['filtered_benign_documents'] = []\n",
    "    for context in adv_documents[key]['benign_responses']:\n",
    "        if context == \"\":\n",
    "            continue\n",
    "        input_text = simple_prompt.format(context=context, question=question)\n",
    "        # running the inference\n",
    "        inputs = g_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        with torch.no_grad():\n",
    "            outputs = g_model.generate(**inputs, max_new_tokens=64)\n",
    "        generated_answer = g_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_answer = generated_answer.split(\"Answer:\")[-1].strip()\n",
    "        # now judge if the generated answer is different from the correct answer\n",
    "        judge_input = judge_prompt.format(generated_answer=generated_answer, original_answer=correct_answer)\n",
    "        judge_inputs = g_tokenizer(judge_input, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        with torch.no_grad():\n",
    "            judge_outputs = g_model.generate(**judge_inputs, max_new_tokens=16)\n",
    "        judge_answer = g_tokenizer.decode(judge_outputs[0], skip_special_tokens=True)\n",
    "        judge_answer = judge_answer.split(\"Answer:\")[-1].strip()\n",
    "        # try to see if the judge answer is Yes\n",
    "        if \"yes\" in judge_answer.lower() and context not in adv_documents[key]['filtered_benign_documents']:\n",
    "            # keep this context\n",
    "            adv_documents[key]['filtered_benign_documents'].append(context)\n",
    "        # if the answer is no or other, we skip\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        del inputs, outputs, judge_inputs, judge_outputs\n",
    "        \n",
    "    # check all adversarial contexts\n",
    "    adv_documents[key]['filtered_adv_documents'] = []\n",
    "    for context in adv_documents[key]['adv_responses']:\n",
    "        if context == \"\":\n",
    "            continue\n",
    "        input_text = simple_prompt.format(context=context, question=question)\n",
    "        # running the inference\n",
    "        inputs = g_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        with torch.no_grad():\n",
    "            outputs = g_model.generate(**inputs, max_new_tokens=64)\n",
    "        generated_answer = g_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_answer = generated_answer.split(\"Answer:\")[-1].strip()\n",
    "        # now judge if the generated answer is different from the correct answer\n",
    "        judge_input = judge_prompt.format(generated_answer=generated_answer, original_answer=incorrect_answer)\n",
    "        judge_inputs = g_tokenizer(judge_input, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        with torch.no_grad():\n",
    "            judge_outputs = g_model.generate(**judge_inputs, max_new_tokens=16)\n",
    "        judge_answer = g_tokenizer.decode(judge_outputs[0], skip_special_tokens=True)\n",
    "        judge_answer = judge_answer.split(\"Answer:\")[-1].strip()\n",
    "        # try to see if the judge answer is Yes\n",
    "        if \"yes\" in judge_answer.lower() and context not in adv_documents[key]['filtered_adv_documents']:\n",
    "            # keep this context\n",
    "            adv_documents[key]['filtered_adv_documents'].append(context)\n",
    "        # if the answer is no or other, we skip\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        del inputs, outputs, judge_inputs, judge_outputs\n",
    "        \n",
    "    del adv_documents[key]['benign_responses']\n",
    "    del adv_documents[key]['adv_responses']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove if the doc is \"\"\n",
    "# with open ('nq_gemini_enrich_10_verify_filtered.json', 'w') as f:\n",
    "#     json.dump(adv_documents, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26003463",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_documents = json.load(open(\"nq_gemini_enrich_10_verify_filtered.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random a golden trigger\n",
    "import random\n",
    "# position is begin, mid and end\n",
    "positions = [\"begin\", \"mid\", \"end\"]\n",
    "num_tokens = [5, 10, 15]\n",
    "target_gradient_guidance = True\n",
    "device = 'cuda:0'\n",
    "p_steps = 10\n",
    "p_target = ['query', 'document']\n",
    "steer = '-1'\n",
    "\n",
    "queries = []\n",
    "for key in adv_documents:\n",
    "    queries.append(adv_documents[key]['question'])\n",
    "benign_docs = []\n",
    "for key in adv_documents:\n",
    "    benign_docs.extend(adv_documents[key]['filtered_benign_documents'])\n",
    "adv_docs = []\n",
    "for key in adv_documents:\n",
    "    adv_docs.extend(adv_documents[key]['filtered_adv_documents'][0])\n",
    "\n",
    "for pos in positions:\n",
    "    for num_token in num_tokens:\n",
    "        # we tried different positions and num_tokens, here we just use one example\n",
    "        # we have only one adv passage, but 10 benign passages, we want to add a trigger both to the adv passages and query to increase its similarity\n",
    "        \n",
    "        # if the p_target is query, we only add trigger to the query\n",
    "        if 'query' in p_target:\n",
    "            for query in queries:\n",
    "                trigger_ids = [e_model.mask_token_id] * num_token\n",
    "                trigger_token_list = e_tokenizer.convert_ids_to_tokens(trigger_ids)\n",
    "                trigger_ids = torch.tensor(trigger_ids, device='cuda:0').unsqueeze(0)\n",
    "                trigger_attention_mask = torch.ones_like(trigger_ids, device='cuda:0')\n",
    "                \n",
    "                query_ids = e_tokenizer(query, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "                if pos == \"begin\":\n",
    "                    p_query_ids = torch.cat([trigger_ids, query_ids['input_ids']], dim=1)\n",
    "                    p_query_attention_mask = torch.cat([trigger_attention_mask, query_ids['attention_mask']], dim=1)\n",
    "                elif pos == \"mid\":\n",
    "                    mid_index = query_ids['input_ids'].size(1) // 2\n",
    "                    p_query_ids = torch.cat([query_ids['input_ids'][:, :mid_index], trigger_ids, query_ids['input_ids'][:, mid_index:]], dim=1)\n",
    "                    p_query_attention_mask = torch.cat([query_ids['attention_mask'][:, :mid_index], trigger_attention_mask, query_ids['attention_mask'][:, mid_index:]], dim=1)\n",
    "                elif pos == \"end\":\n",
    "                    p_query_ids = torch.cat([query_ids['input_ids'], trigger_ids], dim=1)\n",
    "                    p_query_attention_mask = torch.cat([query_ids['attention_mask'], trigger_attention_mask], dim=1)\n",
    "                    \n",
    "                # get the query_embeddings\n",
    "                p_query_embeddings = e_model(input_ids=p_query_ids, attention_mask=p_query_attention_mask)\n",
    "                \n",
    "                # get the document embeddings\n",
    "                b_embeddings = []\n",
    "                for doc in benign_docs:\n",
    "                    doc_ids = e_tokenizer(doc, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "                    doc_embeddings = e_model(input_ids=doc_ids['input_ids'], attention_mask=doc_ids['attention_mask'])\n",
    "                    b_embeddings.append(doc_embeddings)\n",
    "                a_doc_ids = e_tokenizer(adv_docs, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "                a_doc_embeddings = e_model(input_ids=a_doc_ids['input_ids'], attention_mask=a_doc_ids['attention_mask'])\n",
    "            \n",
    "            # iterate p_steps to optimize the trigger\n",
    "            for step in range(p_steps):\n",
    "                # compute similarity loss\n",
    "                b_min_similarities = []\n",
    "                for b_emb in b_embeddings:\n",
    "                    similarity = torch.cosine_similarity(p_query_embeddings.pooler_output, b_emb.pooler_output)\n",
    "                    b_min_similarities.append(similarity)\n",
    "                b_min_sim = torch.min(torch.stack(b_min_similarities))\n",
    "                \n",
    "                a_similarity = torch.cosine_similarity(p_query_embeddings.pooler_output, a_doc_embeddings.pooler_output)\n",
    "                # compute loss\n",
    "                loss = - a_similarity + b_min_sim\n",
    "                # backpropagate\n",
    "                loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe27f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "021e234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "for key in adv_documents:\n",
    "    queries.append(adv_documents[key]['question'])\n",
    "query_ids = e_tokenizer(queries[0], add_special_tokens=False, return_tensors=\"pt\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d53d5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5158, 1657, 17770, 525, 304, 92208, 3940, 3200, 220, 19], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdbc0bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the embedding model to get the embeddings\n",
    "outputs = e_model(**query_ids)\n",
    "embeddings = last_token_pool(outputs.last_hidden_state, query_ids['attention_mask'])\n",
    "embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unknownattack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
