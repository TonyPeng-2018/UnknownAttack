{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa829c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/anp407/.cache/conda_envs/unknownattack/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import colorama\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed, AutoModel\n",
    "from transformers.cache_utils import DynamicCache\n",
    "from datasets import load_dataset\n",
    "import huggingface_hub\n",
    "\n",
    "# Setting reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\" # Tough cookie! Also, requires permissions through HF authentication\n",
    "model_name = \"/u/anp407/Workspace/Huggingface/Qwen/Qwen3-Embedding-0.6B\"\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Attack parameters\n",
    "batch_size = 512 # Number of samples to optimize over (512 in GCG paper)\n",
    "search_batch_size = 256 # Number of samples that actually run forward together\n",
    "top_k = 256 # Number of top tokens to sample from (256 in GCG paper)\n",
    "steps = 100 # Total number of optimization steps (500 in GCG paper)\n",
    "suffix_initial_token = \" !\" # Initial token repeated for the length of the suffix\n",
    "device_name = \"cuda:2\"\n",
    "\n",
    "# Assertions\n",
    "assert batch_size % search_batch_size == 0, \"Batch size must be divisible by search batch size (convenience)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19ad909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model and tokenizer\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_name,\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf1dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "# dataset = load_dataset(\"walledai/AdvBench\", split='train')\n",
    "import json\n",
    "dataset = json.load(open(\"nq_gemini_enrich_10_gemma_verified_final.json\"))\n",
    "\n",
    "# Getting request and target\n",
    "queries = [dataset[x]['question'] for x in dataset]\n",
    "correct_answers = [dataset[x]['correct answer'] for x in dataset]\n",
    "incorrect_answers = [dataset[x]['incorrect answer'] for x in dataset]\n",
    "adv_docs = [dataset[x]['filtered_adv_responses'] + dataset[x]['extra_filtered_adv_responses'] for x in dataset]\n",
    "benign_docs = [dataset[x]['filtered_benign_responses'] + dataset[x]['extra_filtered_benign_responses'] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1704349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [\"start\"]\n",
    "lengths = [1,2,4,8,16]\n",
    "loss_type = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f934bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read existing results if any\n",
    "import os\n",
    "if os.path.exists('gcg_all_result_loss_2.jsonl'):\n",
    "    all_result = {}\n",
    "    with open('gcg_all_result_loss_2.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            for k in entry:\n",
    "                all_result[eval(k)] = entry[k]\n",
    "else:\n",
    "    all_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd010b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.62step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.10step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.87step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.31step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.10step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.01step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.96step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:23<00:00,  4.24step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.35step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.72step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.02step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.87step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:22<00:00,  4.36step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:23<00:00,  4.31step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.70step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.97step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.67step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.97step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:22<00:00,  4.46step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.40step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.94step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.38step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.07step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.06step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.95step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.62step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  2.95step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.62step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.64step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.59step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.57step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.13step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.04step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.47step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.13step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.11step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.15step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.46step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.83step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.72step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  2.96step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.03step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:23<00:00,  4.19step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.53step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.43step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.76step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  2.95step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.85step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.03step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.85step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.99step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.02step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.78step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.54step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.76step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.97step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.16step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.16step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.05step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.46step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.73step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.04step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.07step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.99step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.83step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.47step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.77step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.93step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.98step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.45step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.15step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.13step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.23step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.74step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.55step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.87step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.58step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.94step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.63step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.34step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.51step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.76step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.40step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.04step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.93step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.61step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.53step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.50step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.86step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.06step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.79step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.86step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.17step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.40step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.33step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.40step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.73step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.73step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.05step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:39<00:00,  2.56step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.67step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.81step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.19step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.60step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.12step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.38step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.39step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.05step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.06step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.57step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.41step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  2.98step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  3.02step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.20step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.51step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.64step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.38step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.44step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.76step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.78step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.28step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.36step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.06step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.77step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.58step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:25<00:00,  3.87step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.70step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.07step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.65step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:39<00:00,  2.52step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.76step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:24<00:00,  4.00step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.79step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.49step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.56step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.74step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.67step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.67step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.60step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.65step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.61step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.77step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.56step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.52step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.81step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.49step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.08step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  3.03step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.11step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.62step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.40step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.41step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.77step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.77step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.54step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  3.01step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.16step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.56step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.40step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.77step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:26<00:00,  3.73step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.22step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.14step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.04step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:38<00:00,  2.60step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.42step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.46step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.28step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.40step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.35step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.14step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.33step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.41step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.52step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.08step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.27step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.36step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.27step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.51step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.44step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.25step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.08step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.47step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.15step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:39<00:00,  2.52step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.26step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  2.97step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.41step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.23step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.87step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.41step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.81step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.06step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.14step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.29step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.06step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.85step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.43step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.37step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.41step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.10step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:45<00:00,  2.19step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.24step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.42step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.08step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.05step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.44step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.39step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.29step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.43step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.46step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.41step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.39step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.28step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.34step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.36step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.75step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.45step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  2.95step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.05step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.24step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.23step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.46step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.60step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.29step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.23step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.23step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:31<00:00,  3.22step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.07step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:28<00:00,  3.51step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.44step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.36step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.09step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.79step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.77step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.38step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.28step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.35step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.45step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.42step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  3.02step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.24step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:29<00:00,  3.37step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  3.02step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.11step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:30<00:00,  3.25step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.50step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:32<00:00,  3.07step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:27<00:00,  3.63step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:39<00:00,  2.51step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.42step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:46<00:00,  2.17step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.75step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.34step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.79step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.75step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.83step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.34step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.38step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.45step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.47step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.64step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.73step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:51<00:00,  1.94step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.76step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.43step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:44<00:00,  2.24step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:39<00:00,  2.53step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.74step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.42step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.44step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:45<00:00,  2.19step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.48step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.43step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:43<00:00,  2.30step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.35step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.49step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:45<00:00,  2.20step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.70step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:43<00:00,  2.32step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:44<00:00,  2.24step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.46step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.67step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:39<00:00,  2.51step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.40step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:49<00:00,  2.04step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.72step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.36step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.83step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:48<00:00,  2.04step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:38<00:00,  2.63step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.64step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:49<00:00,  2.03step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:33<00:00,  2.98step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.79step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.65step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.72step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:45<00:00,  2.21step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.38step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:38<00:00,  2.61step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:35<00:00,  2.86step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.48step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:39<00:00,  2.52step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.64step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:38<00:00,  2.58step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:34<00:00,  2.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.39step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:44<00:00,  2.23step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:44<00:00,  2.23step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:49<00:00,  2.03step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.37step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:39<00:00,  2.51step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:48<00:00,  2.05step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:49<00:00,  2.04step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.46step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:36<00:00,  2.72step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.43step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:43<00:00,  2.28step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:43<00:00,  2.32step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.64step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:40<00:00,  2.45step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:46<00:00,  2.13step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:41<00:00,  2.43step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:37<00:00,  2.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:42<00:00,  2.34step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.86step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:56<00:00,  1.76step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:03<00:00,  1.58step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.87step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:51<00:00,  1.93step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.90step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:51<00:00,  1.94step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:50<00:00,  1.99step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:00<00:00,  1.65step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:58<00:00,  1.71step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.90step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.85step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.90step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:09<00:00,  1.45step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.90step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:58<00:00,  1.70step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:01<00:00,  1.61step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.83step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:04<00:00,  1.55step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.81step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:00<00:00,  1.65step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:00<00:00,  1.66step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.92step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:02<00:00,  1.61step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:02<00:00,  1.59step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.81step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.68step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:06<00:00,  1.49step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.67step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:13<00:00,  1.36step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:06<00:00,  1.50step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.79step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.83step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.83step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:07<00:00,  1.47step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:50<00:00,  1.98step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.85step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:09<00:00,  1.44step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.90step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:01<00:00,  1.62step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.67step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.90step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.79step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.86step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.91step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.85step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.82step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.89step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.69step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:01<00:00,  1.63step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:02<00:00,  1.61step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:07<00:00,  1.49step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:58<00:00,  1.70step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.83step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.79step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:06<00:00,  1.49step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:06<00:00,  1.50step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:53<00:00,  1.88step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:52<00:00,  1.90step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:01<00:00,  1.62step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:59<00:00,  1.67step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:01<00:00,  1.63step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:05<00:00,  1.54step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:55<00:00,  1.80step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [00:54<00:00,  1.84step/s]\n",
      "Optimization steps: 100%|██████████| 100/100 [01:00<00:00,  1.65step/s]\n"
     ]
    }
   ],
   "source": [
    "# Running optimization with GCG\n",
    "# for 100 samples\n",
    "# record the best _result\n",
    "# early_stop_thred = 0.1\n",
    "# tolerance_step = 30\n",
    "succeed_step = -1\n",
    "from torch import Tensor \n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    # left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    # if left_padding:\n",
    "    #     return last_hidden_states[:, -1]\n",
    "    # else:\n",
    "    sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "    batch_size = last_hidden_states.shape[0]\n",
    "    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "for position in positions:\n",
    "    for length in lengths:\n",
    "        for i, query in enumerate(queries):\n",
    "            succeed_step = -1\n",
    "            if (i, position, length) in all_result:\n",
    "                print(f\"Skipping already processed (i={i}, position={position}, length={length})\")\n",
    "                continue\n",
    "            # check the rerun label\n",
    "\n",
    "            all_result[(i, position, length, loss_type)] = {}\n",
    "            all_result[(i, position, length, loss_type)]['loss_history'] = []\n",
    "            all_result[(i, position, length, loss_type)]['ids_suffix_history'] = []\n",
    "            \n",
    "            # Fix variable shadowing and indexing\n",
    "            current_adv_docs = adv_docs[i]\n",
    "            current_benign_docs = benign_docs[i]\n",
    "            \n",
    "            # The user code had: adv_docs = adv_docs[i]  # only use the first adv doc for now\n",
    "            # Assuming they want the first document from the list of documents for this query\n",
    "            if isinstance(current_adv_docs, list) and len(current_adv_docs) > 0:\n",
    "                target_adv_docs = [current_adv_docs[0]]\n",
    "            else:\n",
    "                target_adv_docs = [current_adv_docs] if isinstance(current_adv_docs, str) else current_adv_docs\n",
    "\n",
    "            if isinstance(current_benign_docs, list) and len(current_benign_docs) > 0:\n",
    "                target_benign_docs = current_benign_docs # Use all?\n",
    "            else:\n",
    "                target_benign_docs = [current_benign_docs] if isinstance(current_benign_docs, str) else current_benign_docs\n",
    "            \n",
    "            if position == \"start\":\n",
    "                text_before = \"\"\n",
    "                text_mid = \" !\" * length\n",
    "                text_after = query\n",
    "            elif position == \"mid\":\n",
    "                text_before = query[:len(query)//2]\n",
    "                text_mid = \" !\" * length\n",
    "                text_after = query[len(query)//2:]\n",
    "            elif position == \"end\":\n",
    "                text_before = query\n",
    "                text_mid = \" !\" * length\n",
    "                text_after = \"\"\n",
    "\n",
    "            ids_before = tokenizer(text_before, return_tensors=\"pt\", add_special_tokens=False).to(model.device)['input_ids']\n",
    "            ids_mid = tokenizer(text_mid, return_tensors=\"pt\", add_special_tokens=False).to(model.device)['input_ids']\n",
    "            ids_after = tokenizer(text_after, return_tensors=\"pt\", add_special_tokens=False).to(model.device)['input_ids']\n",
    "            \n",
    "            # Initialize suffix ids from ids_mid\n",
    "            ids_suffix = ids_mid.clone()\n",
    "\n",
    "            # Pre-compute doc embeddings\n",
    "            adv_doc_ids = tokenizer(target_adv_docs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)['input_ids']\n",
    "            with torch.no_grad():\n",
    "                adv_doc_embeds = last_token_pool(model(adv_doc_ids, output_hidden_states=True).last_hidden_state, adv_doc_ids != tokenizer.pad_token_id)\n",
    "            \n",
    "            benign_doc_ids = tokenizer(target_benign_docs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)['input_ids']\n",
    "            with torch.no_grad():\n",
    "                benign_doc_embeds = last_token_pool(model(benign_doc_ids, output_hidden_states=True).last_hidden_state, benign_doc_ids != tokenizer.pad_token_id)\n",
    "            ids_suffix_best = ids_suffix.clone()\n",
    "            best_loss = float(\"inf\")\n",
    "            all_losses = []\n",
    "            \n",
    "            for step in tqdm(range(steps), desc=\"Optimization steps\", unit=\"step\"):\n",
    "                # 1. Gradient Step\n",
    "                embeds_before = model.get_input_embeddings()(ids_before)\n",
    "                embeds_after = model.get_input_embeddings()(ids_after)\n",
    "                \n",
    "                one_hot = torch.nn.functional.one_hot(ids_suffix, num_classes=model.config.vocab_size).to(model.device, model.dtype)\n",
    "                one_hot.requires_grad = True\n",
    "                embeds_suffix = one_hot @ model.get_input_embeddings().weight\n",
    "                \n",
    "                full_embeds = torch.cat([embeds_before, embeds_suffix, embeds_after], dim=1)\n",
    "                \n",
    "                outputs = model(inputs_embeds=full_embeds, output_hidden_states=True)\n",
    "                q_embeds = last_token_pool(outputs.last_hidden_state, ids_before != tokenizer.pad_token_id)\n",
    "                \n",
    "                # plan 0s: simple simliarity difference\n",
    "                if loss_type == 0:\n",
    "                    sim_adv = torch.cosine_similarity(q_embeds, adv_doc_embeds, dim=-1).min() # more the better\n",
    "                    sim_benign = torch.cosine_similarity(q_embeds, benign_doc_embeds, dim=-1).max() # less the better\n",
    "                    loss = - sim_adv + sim_benign\n",
    "                # print(f\"Step {step}: Loss = {loss.item()}, Sim_adv = {sim_adv.item()}, Sim_benign = {sim_benign.item()}\")\n",
    "                # plan1: contrastive loss contrastive learning loss https://lilianweng.github.io/posts/2021-05-31-contrastive/ \n",
    "                elif loss_type == 1:\n",
    "                    temp = 0.07 # use common temperature https://openreview.net/forum?id=ejHUr4nfHhD\n",
    "                    sim_adv = torch.cosine_similarity(q_embeds, adv_doc_embeds, dim=-1)\n",
    "                    sim_benign = torch.cosine_similarity(q_embeds, benign_doc_embeds, dim=-1)\n",
    "                    logits = torch.cat([sim_adv, sim_benign], dim=0) / temp # sim_adv more the better, sim_benign less the better\n",
    "                    labels = torch.zeros(logits.size(0), dtype=torch.float).to(model.device)\n",
    "                    labels[0] += 1  # First half are adv, second half benign\n",
    "                    loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "                # plan2: qwen training loss https://arxiv.org/pdf/2506.05176.pdf\n",
    "                elif loss_type == 2:\n",
    "                    temp = 0.07\n",
    "                    sim_adv = torch.cosine_similarity(q_embeds, adv_doc_embeds, dim=-1)/ temp\n",
    "                    sim_benign = torch.cosine_similarity(q_embeds, benign_doc_embeds, dim=-1)/ temp\n",
    "                    sim_adv_benign = torch.cosine_similarity(adv_doc_embeds, benign_doc_embeds, dim=-1)/ temp # fixed\n",
    "                    sim_benign_exp_mean = torch.exp(sim_benign).mean(dim=0) # less the better\n",
    "                    sim_adv_benign_exp_mean = torch.exp(sim_adv_benign).mean(dim=0) # fixed\n",
    "                    sim_adv_exp_mean = torch.exp(sim_adv).mean(dim=0) # more the better\n",
    "                    Z = sim_adv_exp_mean + sim_benign_exp_mean + sim_adv_benign_exp_mean\n",
    "                    loss = - torch.log(sim_adv_exp_mean / Z) \n",
    "                # plan 3: triplet loss\n",
    "                # margin = 0.1\n",
    "                # loss = torch.clamp(margin - sim_adv + sim_benign, min=0.0) # want sim_benign - sim_adv = 0.1\n",
    "                # plan 4: nce loss\n",
    "                # temp = 0.07\n",
    "                # logits = torch.mm(q_embeds, torch.cat([adv_doc_embeds, benign_doc_embeds], dim=0).t()) / temp\n",
    "                # labels = torch.zeros(q_embeds.size(0), dtype=torch.long).to(model.device)\n",
    "                # loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "                # plan 5: margin ranking loss\n",
    "                # margin = 0.1\n",
    "                # target = torch.ones_like(sim_adv)\n",
    "                # loss = torch.nn.MarginRankingLoss(margin=margin)(sim_adv.unsqueeze(1), sim_benign.unsqueeze(1), target)\n",
    "\n",
    "                loss.backward()\n",
    "                gradients = -one_hot.grad\n",
    "                \n",
    "                all_losses.append(loss.item())\n",
    "                if loss.item() < best_loss:\n",
    "                    best_loss = loss.item()\n",
    "                    ids_suffix_best = ids_suffix.clone()\n",
    "\n",
    "                # 2. Candidate Selection\n",
    "                top_k_tokens = torch.topk(gradients, top_k, dim=-1).indices\n",
    "                \n",
    "                sub_positions = torch.randint(0, length, (batch_size,))\n",
    "                sub_tokens = torch.randint(0, top_k, (batch_size,))\n",
    "                \n",
    "                candidate_ids = ids_suffix.clone().repeat(batch_size, 1)\n",
    "                for idx, (pos, tok_idx) in enumerate(zip(sub_positions, sub_tokens)):\n",
    "                    candidate_ids[idx, pos] = top_k_tokens[0, pos, tok_idx]\n",
    "                \n",
    "                candidate_losses = []\n",
    "                for slice_start in range(0, batch_size, search_batch_size):\n",
    "                    slice_end = min(slice_start + search_batch_size, batch_size)\n",
    "                    ids_slice = candidate_ids[slice_start: slice_end]\n",
    "                    \n",
    "                    curr_batch_size = ids_slice.shape[0]\n",
    "                    batch_ids_before = ids_before.repeat(curr_batch_size, 1)\n",
    "                    batch_ids_after = ids_after.repeat(curr_batch_size, 1)\n",
    "                    batch_input_ids = torch.cat([batch_ids_before, ids_slice, batch_ids_after], dim=1)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        batch_outputs = model(input_ids=batch_input_ids, output_hidden_states=True)\n",
    "                        batch_q_embeds = last_token_pool(batch_outputs.last_hidden_state, batch_input_ids != tokenizer.pad_token_id)\n",
    "                        \n",
    "                        batch_q_norm = torch.nn.functional.normalize(batch_q_embeds, p=2, dim=1)\n",
    "                        adv_norm = torch.nn.functional.normalize(adv_doc_embeds, p=2, dim=1)\n",
    "                        benign_norm = torch.nn.functional.normalize(benign_doc_embeds, p=2, dim=1)\n",
    "                        \n",
    "                        batch_sim_adv = torch.mm(batch_q_norm, adv_norm.t())\n",
    "                        batch_sim_benign = torch.mm(batch_q_norm, benign_norm.t())\n",
    "                        \n",
    "                        batch_loss = - batch_sim_adv.min(dim=1).values + batch_sim_benign.max(dim=1).values\n",
    "                        candidate_losses.extend(batch_loss.tolist())\n",
    "\n",
    "                best_idx = np.argmin(candidate_losses)\n",
    "                # print(f\"Step {step}: Best Loss = {candidate_losses[best_idx]}\")\n",
    "                ids_suffix = candidate_ids[best_idx].unsqueeze(0)\n",
    "                \n",
    "                min_loss = np.min(candidate_losses)\n",
    "                # if the candidate_loss is less than 0, then success and break.\n",
    "                if min_loss < 0:\n",
    "                    succeed_step = step\n",
    "                \n",
    "                all_result[(i, position, length, loss_type)]['loss_history'].append(min_loss.item())\n",
    "                all_result[(i, position, length, loss_type)]['ids_suffix_history'].append(ids_suffix.cpu().numpy())\n",
    "                all_result[(i, position, length, loss_type)]['succeed_step'] = succeed_step\n",
    "                \n",
    "                # if len(all_result[(i, position, length)]['loss_history']) > tolerance_step:\n",
    "                #     recent_losses = all_result[(i, position, length)]['loss_history'][-tolerance_step:]\n",
    "                #     if (max(recent_losses) - min(recent_losses))/ max(recent_losses) < early_stop_thred:\n",
    "                #         print(f\"Early stopping at step {step} due to minimal loss improvement.\")\n",
    "                #         break\n",
    "            \n",
    "                del embeds_before, embeds_suffix, embeds_after, full_embeds, outputs, one_hot, gradients, candidate_ids, candidate_losses\n",
    "                torch.cuda.empty_cache()\n",
    "                    \n",
    "            all_result[(i, position, length, loss_type)]['best_loss'] = best_loss\n",
    "            all_result[(i, position, length, loss_type)]['ids_suffix_best'] = ids_suffix_best.cpu().numpy()\n",
    "            # save the result\n",
    "            with open(f'gcg_all_result.jsonl', 'a') as f:\n",
    "                for sub_k in all_result[(i, position, length, loss_type)]:\n",
    "                    if torch.is_tensor(all_result[(i, position, length, loss_type)][sub_k]):\n",
    "                        all_result[(i, position, length, loss_type)][sub_k] = all_result[(i, position, length, loss_type)][sub_k].cpu().tolist()\n",
    "                    if isinstance(all_result[(i, position, length, loss_type)][sub_k], np.ndarray):\n",
    "                        all_result[(i, position, length, loss_type)][sub_k] = all_result[(i, position, length, loss_type)][sub_k].tolist()\n",
    "                    # if list\n",
    "                    if isinstance(all_result[(i, position, length, loss_type)][sub_k], list):\n",
    "                        for idx, item in enumerate(all_result[(i, position, length, loss_type)][sub_k]):\n",
    "                            if torch.is_tensor(item):\n",
    "                                all_result[(i, position, length, loss_type)][sub_k][idx] = item.cpu().tolist()\n",
    "                            if isinstance(item, np.ndarray):\n",
    "                                all_result[(i, position, length, loss_type)][sub_k][idx] = item.tolist()\n",
    "                f.write(json.dumps({str((i, position, length, loss_type)): all_result[(i, position, length, loss_type)]}) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unknownattack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
